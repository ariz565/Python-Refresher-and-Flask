# Iterator is an object that contains a countable number of values and can be iterated upon, meaning that you can traverse through all the values. In Python, an iterator is an object which implements the iterator protocol, which consists of the methods __iter__() and __next__().

# __iter__() returns the iterator object itself and is used in cases where an iterable needs to be converted into an iterator. 
# __next__() returns the next value from the iterator. When there are no more items to return, it raises the StopIteration exception.

# so an iterator is any object we can loop over with for or call a next() function on.
# nums = [1,23,4]
# it = iter(nums)

# print(next(it))
# print(next(it))
# print(next(it))
# print(next(it))  # This will raise StopIteration

# syntax: iter([1,2,3,4])  # returns an iterator object

# Generator
# Generator is a special type of iterator that is defined using a function rather than a class. A generator function uses the yield statement to produce a series of values over time, pausing after each yield and resuming from where it left off when next() is called again.
# Generators automatically implements the iterator protocol, so you don't need to define __iter__() and __next__() methods explicitly.
# they are lazy, meaning they generate values on the fly and do not store them in memory, making them more memory efficient for large datasets.

# def my_gen():
#     yield 1
#     yield 2
#     yield 3

# g = my_gen()

# print(next(g))  # 1
# print(next(g))  # 2
# print(next(g))  # 3

# syntax: ( x*x for x in range(5))

# class FibonacciIterator:
#     def __init__(self, limit):
#         self.limit = limit
#         self.a, self.b = 0, 1
#         self.count = 0

#     def __iter__(self):
#         return self

#     def __next__(self):
#         if self.count >= self.limit:
#             raise StopIteration
#         self.count += 1
#         value = self.a
#         self.a, self.b = self.b, self.a + self.b
#         return value

# # Usage
# fib = FibonacciIterator(5)
# for num in fib:
#     print(num)

# generator function for fibo

# def fibonacci(limit):
#     a,b = 0,1
#     for _ in range(limit):
#         yield a
#         a,b = b, a + b

# for num in fibonacci(5):
#     print(num)

# Both iterators and generators are lazy, but generators are a shorthand way of writing iterators. For example, Fibonacci as an iterator class requires state variables and StopIteration, whereas the generator version just uses yield. So, generators make code more concise and Pythonic
# iterators must implement both __iter__ and __next__ and generator just use yeild

### DECORATORS
# A decorator is a function that wraps another function to extend its behavior without changing its code. It’s widely used for logging, authentication, retries, and frameworks like Flask and Django rely heavily on decorators
# Python provides built-in decorators: @staticmethod, @classmethod, @property.
# For preserving original function metadata, use functools.wraps

# from functools import wraps
# def my_decorator(func):
#     @wraps(func)
#     def wrapper():
#         print("Before the function runs")
#         func()
#         print("After the function runs")
#     return wrapper

## Example
# def my_decorator(func):
#     def wrapper():
#         print("Before the function runs")
#         func()
#         print("After the function runs")
#     return wrapper

# @my_decorator
# def hello():
#     print("Hello world")

# hello()

## CLASS BASED DECORATOR
# class MyDecorator:
#     def __init__(self, func):
#         self.func = func   # store the function being decorated

#     def __call__(self, *args, **kwargs):
#         print("Before the function runs...")
#         result = self.func(*args, **kwargs)  # call the original function
#         print("After the function runs...")
#         return result


# @MyDecorator
# def greet(name):
#     print(f"Hello, {name}!")

# greet("Alice")

## Built in decorators
# @staticmethod - method that doesn't receive an implicit first argument (like self or cls). Its behavior is similar to a regular function but lives in the class's namespace.
# class MathUtils:
#     @staticmethod
#     def add(a, b):
#         return a + b

# print(MathUtils.add(2, 3))  # 5

# @classmethod - method that receives the class as the first argument (cls).
# class Person:
#     def __init__(self, name):
#         self.name = name

#     @classmethod
#     def from_string(cls, name_str):
#         return cls(name_str)

# p = Person.from_string("Alice")
# print(p.name)  # Alice

# @property - allows you to define methods in a class that can be accessed like attributes. It is used to manage the access to instance attributes, often for validation or computed properties.
# Used to define getters and setters in a clean way (without calling like obj.get_x()).
# class Circle:
#     def __init__(self, radius):
#         self._radius = radius

#     @property
#     def radius(self):
#         return self._radius

#     @radius.setter
#     def radius(self, value):
#         if value < 0:
#             raise ValueError("Radius must be positive")
#         self._radius = value


# c = Circle(5)
# print(c.radius)  # 5
# c.radius = 10    # calls setter
# print(c.radius)  # 10
# # c.radius = -3   # raises ValueError

### ARGS AND KWARGS

# *args = collects positional arguments into a tuple.
# **kwargs = collects keyword arguments into a dictionary.

# def demo(*args, **kwargs):
#     print("args:", args)
#     print("kwargs:", kwargs)

# demo(1, 2, 3, name="Alice", age=25)


### CONTEXT MANAGERS
# -->
# Context managers are a way to allocate and release resources precisely when you want to. The most common example is the with statement used for file operations. Context managers ensure that resources are properly cleaned up after use, even if an error occurs.
# You can create a context manager using a class with __enter__ and __exit__ methods

# class MyContextManager:
#     def __enter__(self):
#         print("Entering the context...")
#         return self

#     def __exit__(self, exc_type, exc_value, traceback):
#         print("Exiting the context...")
#         if exc_type:
#             print(f"An error occurred: {exc_value}")
#         return True  # Suppress exceptions

# with MyContextManager() as cm:
#     print("Inside the context...")
#     # Uncomment the next line to see exception handling in action
#     # raise ValueError("Something went wrong!")
#     print("Doing some work...")

# You can also create a context manager using the contextlib module, which provides a simpler way to define context managers using generator functions.

## how does python manage memory and how does the garbage collector work
#-->
# Python uses a combination of reference counting and a cyclic garbage collector to manage memory. Each object in Python has a reference count that tracks how many references point to it. When an object's reference count drops to zero, it is immediately deallocated.
# However, reference counting alone cannot handle cyclic references (where two or more objects reference each other). To address this, Python includes a cyclic garbage collector that periodically looks for groups of objects that reference each other but are no longer reachable from the program. When such cycles are found, the garbage collector breaks the cycles and frees the memory.
# You can manually trigger garbage collection using the gc module, but it's generally not necessary as Python's garbage collector runs automatically.

# Python uses a private heap space to manage memory. All Python objects and data structures are stored in this heap, and the Python memory manager takes care of allocating and deallocating memory as needed. The memory manager uses techniques like reference counting and garbage collection to ensure efficient memory usage and to free up memory that is no longer needed.
# To handle reference cycles, Python has a cyclic garbage collector (gc module).
# It runs periodically and looks for unreachable objects in cycles.
# It uses generational GC:
# Objects are divided into three generations (young → old).
# Most objects die young, so the GC checks younger generations more often.
# Surviving objects get “promoted” to older generations.
# This reduces overhead since older objects are less likely to be garbage.

## How does Python handle memory management for large data structures like lists and dictionaries?
#-->
# Python manages memory for large data structures like lists and dictionaries using dynamic arrays and hash tables, respectively. When you create a list, Python allocates a contiguous block of memory to store the elements. If the list grows beyond its current capacity, Python allocates a larger block of memory, copies the existing elements to the new block, and frees the old block. This resizing operation is amortized, meaning that while it can be costly when it happens, it doesn't happen often relative to the number of insertions.
# For dictionaries, Python uses a hash table to store key-value pairs. When you add a new key-value pair, Python computes a hash of the key to determine where to store the value in the table. If there are collisions (two keys hash to the same index), Python uses open addressing or chaining to resolve them. Like lists, if the dictionary grows too large, Python will resize the hash table to maintain efficient access times.
# Both lists and dictionaries in Python are designed to be efficient in terms of memory usage and access times, even as they grow in size.

## What are some common memory management pitfalls in Python, and how can they be avoided?  
#-->
# Common memory management pitfalls in Python include memory leaks, excessive memory usage, and reference cycles.
# To avoid memory leaks, be cautious with global variables and long-lived objects that hold references to large data structures. Using weak references (via the weakref module) can help prevent unintended retention of objects.
# To manage excessive memory usage, consider using generators instead of lists for large datasets, as generators yield items one at a time and do not store the entire dataset in memory.
# To avoid reference cycles, be mindful of creating circular references between objects. If you do create cycles, ensure that the objects involved have a __del__ method that properly cleans up resources, or use the gc module to manually trigger garbage collection when necessary.
# Additionally, using context managers (with the with statement) can help ensure that resources are properly released when they are no longer needed.
# Using tools like memory profilers (e.g., memory_profiler, objgraph) can help identify memory usage patterns and potential leaks in your code.     

## Global Interpreter Lock (GIL) in Python
#-->
# The Global Interpreter Lock (GIL) is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. This means that even in a multi-threaded Python program, only one thread can execute Python code at a time. The GIL is necessary because CPython's memory management is not thread-safe.
# The GIL can be a bottleneck in CPU-bound multi-threaded programs, as threads may spend time waiting for the GIL to be released. However, it does not affect I/O-bound programs as much, since threads can release the GIL while waiting for I/O operations to complete.
# To work around the GIL for CPU-bound tasks, you can use the multiprocessing module, which creates separate processes with their own Python interpreter and memory space, allowing true parallelism. Alternatively, you can use implementations of Python that do not have a GIL, such as Jython or IronPython, or use libraries like Cython to write performance-critical code in C.
# The GIL is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously.
# It simplifies memory management but can be a bottleneck in CPU-bound multi-threaded programs. 
# For I/O-bound tasks, the GIL is less of an issue since threads can release it while waiting for I/O operations.
# To bypass the GIL for CPU-bound tasks, use the multiprocessing module to create separate processes with their own Python interpreter and memory space.
# Alternatively, use implementations of Python without a GIL (like Jython or IronPython) or libraries like Cython for performance-critical code.
# The GIL is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously.

# CPython uses reference counting for memory management.
# Updating reference counts must be thread-safe.
# Instead of making every memory operation thread-safe (which would be expensive), Python uses the GIL to protect the entire interpreter.

# GIL switching: The interpreter periodically switches threads every X bytecode instructions or after some time.

# In CPython 3.2+, the default check interval is 5ms (configurable).
# This means that a thread can hold the GIL for up to 5ms before another thread gets a chance to run.

## Python vs Java vs JavaScript
#-->
# Python is dynamically typed, interpreted, and has a simple syntax, making it great for rapid development and scripting. It has a vast standard library and is widely used in data science, web development, and automation.
# Java is statically typed, compiled to bytecode, and runs on the Java Virtual Machine (JVM). It is known for its performance, portability, and strong ecosystem, making it popular for large-scale enterprise applications and Android development.
# JavaScript is dynamically typed, interpreted, and primarily used for client-side web development. It has evolved to support server-side development (Node.js) and has a rich ecosystem of libraries and frameworks for building interactive web applications.

### Python (CPython) – Global Interpreter Lock (GIL)
# Only one thread runs Python bytecode at a time (GIL).
# Good for I/O-bound tasks, not for CPU-bound parallelism.
# For CPU-bound tasks, use multiprocessing (each process has its own GIL).
# Concurrency: multi-threading exists, but limited by GIL.

### Java – True Multi-threading
# No GIL; JVM allows true parallel execution of threads across CPU cores.
# Uses OS-level threads and strong concurrency primitives (synchronized, volatile, ReentrantLock, Executors).
# Best for CPU-bound parallel tasks.
# Memory safety via explicit synchronization, not a global lock.
# On multi-core CPUs, threads run in parallel.

### JavaScript – Single-threaded with Event Loop
# Single-threaded (Node.js/browser); no GIL needed.
# Concurrency via event loop, async callbacks, promises, async/await.
# For CPU-bound tasks: use Web Workers (browser) or Worker Threads/clustering (Node.js).
# Best for I/O-bound, non-blocking, highly scalable tasks.

### Java Execution Model
# Compiled + Interpreted (Hybrid via JVM)
# Write code → .java file
# Compile: javac → bytecode (.class)
# Bytecode is platform-independent (runs anywhere JVM exists)
# JVM loads classes dynamically (ClassLoader), verifies bytecode
# Execution: interpreted at first, then JIT compiles hot code to native machine code
# Garbage Collector (GC) manages heap (young/old generations)
# Thread model: true OS threads, concurrency via synchronization/locks
# Output: native machine instructions
# Summary: Java = compiled to bytecode → JVM interprets/JIT compiles → runs on OS/CPU

### Python Execution Model
# Interpreted (with Bytecode + VM + GIL constraint)
# Write code → .py file
# Compiled to bytecode (.pyc in __pycache__)
# Python Virtual Machine (PVM) executes bytecode instruction by instruction (stack-based VM)
# GIL: only one thread executes bytecode at a time
# Multithreading possible, but CPU-bound tasks limited; use multiprocessing to bypass GIL
# Garbage Collection: primary = reference counting, secondary = cyclic GC
# Output: executed via C functions at runtime
# Summary: Python = interpreted (bytecode + VM), single-threaded (GIL), dynamic, slower than Java

### JavaScript Execution Model
# Interpreted + JIT (V8, SpiderMonkey, etc.) with Event Loop
# Write code → .js file
# Parsing: JS engine parses code, creates AST
# Bytecode generated, hot code optimized to native machine code by JIT
# Single-threaded execution (browser/Node.js), uses call stack
# Event loop + callback queue for async tasks (timers, I/O, promises)
# Async tasks handled by browser/Node APIs, results queued for event loop
# Memory management: garbage collection (Mark-and-Sweep)
# Output: native instructions executed by CPU
# Summary: JavaScript = JIT-compiled, single-threaded with async concurrency (event loop), great for I/O-bound apps

# JIT
# Just-In-Time compilation (JIT) is a technique used to improve the performance of interpreted languages like Java and JavaScript. In JIT compilation, the code is compiled into machine code at runtime, just before it is executed, rather than being compiled ahead of time. This allows the JIT compiler to optimize the code based on the actual execution context, such as the types of variables and the frequency of function calls.

# is python single threaded
# Python (specifically CPython) is often described as single-threaded due to the Global Interpreter Lock (GIL). The GIL is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. This means that even in a multi-threaded Python program, only one thread can execute Python code at a time.
# However, Python does support multi-threading, and threads can be used for I/O-bound tasks where the program spends time waiting for external resources (like file I/O or network requests). In such cases, threads can release the GIL while waiting, allowing other threads to run.
# For CPU-bound tasks that require true parallelism, Python provides the multiprocessing module, which creates separate processes with their own Python interpreter and memory space, allowing multiple processes to run simultaneously on multiple CPU cores.

# asyncio
# asyncio is a library in Python used for writing concurrent code using the async/await syntax. It is designed for I/O-bound and high-level structured network code. asyncio provides a way to write asynchronous programs that can handle many tasks at once without using threads or processes. It uses an event loop to manage and schedule tasks, allowing the program to perform other operations while waiting for I/O operations to complete. This makes asyncio particularly well-suited for applications that require high concurrency, such as web servers or network clients.

# Dunder methods
# Dunder methods, short for "double underscore" methods, are special methods in Python that have names that start and end with double underscores (e.g., __init__, __str__, __add__). These methods are also known as magic methods or special methods. They allow you to define the behavior of your custom classes for built-in operations, such as initialization, string representation, arithmetic operations, and more. By implementing dunder methods in your classes, you can make your objects behave like built-in types and integrate seamlessly with Python's syntax and features.
# Examples of common dunder methods include:
# __init__(self, ...): Constructor method called when an object is created.
# __str__(self): Returns a string representation of the object (used by str() and print()).
# __repr__(self): Returns a detailed string representation of the object (used by repr())
# __add__(self, other): Defines behavior for the + operator.
# __len__(self): Returns the length of the object (used by len()).
# __eq__(self, other): Defines behavior for the == operator.

# Example
# class Point:
#     def __init__(self, x, y):
#         self.x = x
#         self.y = y

#     def __str__(self):
#         return f"Point({self.x}, {self.y})"

#     def __add__(self, other):
#         if isinstance(other, Point):
#             return Point(self.x + other.x, self.y + other.y)
#         return NotImplemented

#     def __eq__(self, other):
#         if isinstance(other, Point):
#             return self.x == other.x and self.y == other.y
#         return NotImplemented
    
# p1 = Point(2, 3)
# p2 = Point(4, 5)
# print(p1)  # Output: Point(2, 3)
# p3 = p1 + p2
# print(p3)  # Output: Point(6, 8)
# print(p1 == p2)  # Output: False
# print(p1 == Point(2, 3))  # Output: True

# dataclasses
# Dataclasses are a feature in Python that provides a decorator and functions for automatically adding special methods to user-defined classes. Introduced in Python 3.7 via the dataclasses module, they simplify the process of creating classes that primarily store data by automatically generating methods like __init__(), __repr__(), __eq__(), and others based on class attributes.
# To use dataclasses, you decorate a class with @dataclass and define its attributes using type annotations. This reduces boilerplate code and makes the class definition cleaner and more readable.
# Example:
# from dataclasses import dataclass

# @dataclass
# class Point:
#     x: int
#     y: int  
#     label: str = "A point"  # default value
#     def move(self, dx: int, dy: int):
#         self.x += dx
#         self.y += dy
# p1 = Point(2, 3)
# p2 = Point(4, 5, label="Another point")
# print(p1)  # Output: Point(x=2, y=3, label='A point')
# print(p2)  # Output: Point(x=4, y=5, label='Another point')
# p1.move(1, -1)      
# print(p1)  # Output: Point(x=3, y=2, label='A point')
# print(p1 == Point(3, 2))  # Output: True

## answers to interview questions
# self , cls, this, me, @classmethod, @staticmethod, super(), __init__, __new__, __str__, __repr__, __dict__, __slots__, __call__, __getattr__, __setattr__, __delattr__, __iter__, __next__, __len__, __getitem__, __setitem__, __delitem__, __contains__, __add__, __sub__, __mul__, __truediv__, __floordiv__, __mod__, __pow__, __lt__, __le__, __eq__, __ne__, __gt__, __ge__, __enter__, __exit__, __copy__, __deepcopy__, __hash__, __bool__, __dir__, __format__, __sizeof__, __reduce__, __reduce_ex__, __getstate__,

# self is a reference to the current instance of the class. It is used to access variables and methods associated with the instance. In instance methods, self must be the first parameter.
# cls is a reference to the class itself, rather than an instance of the class. It is used in class methods, which are methods that operate on the class as a whole rather than on individual instances. In class methods, cls must be the first parameter.
# this and me are not used in Python. They are commonly used in other programming languages like Java and C# to refer to the current instance of a class, but in Python, self serves this purpose.
# @classmethod is a decorator that indicates that a method is a class method. Class methods receive the class (cls) as the first argument instead of the instance (self). They can be called on the class itself or on instances of the class.
# @staticmethod is a decorator that indicates that a method is a static method. Static methods do not receive an implicit first argument (neither self nor cls). They behave like regular functions but belong to the class's namespace.
# super() is a built-in function that allows you to call methods from a parent class. It is commonly used in inheritance to access and extend the functionality of the parent class.
# __init__ is a special method called a constructor. It is automatically invoked when an instance of a class is created. It is used to initialize the instance's attributes.
# __new__ is a special method that is responsible for creating a new instance of a class. It is called before __init__ and is rarely overridden unless you need to customize instance creation.
# __str__ is a special method that defines the string representation of an object. It is called by the str() function and the print() function to provide a human-readable representation of the object.
# __repr__ is a special method that defines the official string representation of an object. It is called by the repr() function and is intended to provide a detailed and unambiguous representation of the object, often useful for debugging.
# __dict__ is an attribute that stores an object's writable attributes in a dictionary. It allows you to access and modify the attributes of an object dynamically.
# __slots__ is a special attribute that can be defined in a class to restrict the attributes that instances of the class can have. It can help save memory by preventing the creation of a __dict__ for each instance.
# __call__ is a special method that allows an instance of a class to be called as a function. When you call an instance, the __call__ method is invoked.
# __getattr__ is a special method that is called when an attribute that does not exist is accessed. It allows you to define custom behavior for handling missing attributes.


## Functional programming concepts in Python#-->
# Functional programming is a programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing state or mutable data. In Python, functional programming concepts can be implemented using first-class functions, higher-order functions, pure functions, and immutability.
# First-class functions mean that functions in Python can be treated like any other variable. They can be passed as arguments to other functions, returned from functions, and assigned to variables.   
# Higher-order functions are functions that can take other functions as arguments or return functions as results. Examples of higher-order functions in Python include map(), filter(), and reduce().
# Pure functions are functions that always produce the same output for the same input and do not have side effects (i.e., they do not modify any external state). Pure functions are easier to reason about and test.
# Immutability refers to the concept of data that cannot be changed after it is created. In functional programming, it is common to use immutable data structures, such as tuples and frozensets, to avoid unintended side effects.
# Functional programming emphasizes the use of recursion and avoids traditional looping constructs like for and while loops.# List comprehensions and generator expressions are also commonly used in functional programming to create new lists or generators based on existing iterables in a concise and readable way.

# Map, filter, reduce
# Map, filter, and reduce are higher-order functions in Python that allow you to apply functions to iterables in a functional programming style.
# map() applies a given function to each item in an iterable (like a list) and returns an iterator with the results. It is often used for transforming data.
# Example:
# nums = [1, 2, 3, 4]
# squared = map(lambda x: x**2, nums)
# print(list(squared))  # Output: [1, 4, 9, 16]
# filter() applies a given function to each item in an iterable and returns an iterator with only the items for which the function returns True. It is often used for filtering data.
# Example:
# nums = [1, 2, 3, 4]
# even = filter(lambda x: x % 2 == 0, nums)
# print(list(even))  # Output: [2, 4]
# reduce() applies a given function cumulatively to the items in an iterable, from left to right, so as to reduce the iterable to a single value. It is often used for aggregating data.    
# from functools import reduce
# nums = [1, 2, 3, 4]
# product = reduce(lambda x, y: x * y, nums)
# print(product)  # Output: 24

# List comprehensions and generator expressions
# List comprehensions and generator expressions are concise ways to create lists and generators in Python, respectively.
# List comprehensions provide a compact syntax for generating lists by applying an expression to each item in an iterable, optionally filtering items with a condition. They are enclosed in square brackets [].
# Example:
# nums = [1, 2, 3, 4]
# squared = [x**2 for x in nums]
# print(squared)  # Output: [1, 4, 9, 16]
# Generator expressions are similar to list comprehensions but use parentheses () instead of square brackets []. They return an iterator that generates items on-the-fly and are more memory-efficient for large datasets.
# Example:
# nums = [1, 2, 3, 4]
# squared_gen = (x**2 for x in nums)
# print(list(squared_gen))  # Output: [1, 4, 9, 16]
# Example:
# nums = [1, 2, 3, 4, 5, 6]
# even_squares = [x**2 for x in nums if x % 2 == 0]
# print(even_squares)  # Output: [4, 16, 36]    

## Metaclasses and Descriptors: Gaining a deep understanding of Python's object model and how to customize class creation and attribute access.
#-->
# Metaclasses are classes of classes that define how classes behave. A metaclass is responsible for creating a class, and you can customize the class creation process by defining a metaclass. By default, Python uses type as the metaclass for all classes, but you can create your own metaclasses by inheriting from type and overriding methods like __new__() and __init__().
# Descriptors are a way to manage the attributes of a class. A descriptor is an object that defines how attribute access is managed through methods like __get__(), __set__(), and __delete__(). Descriptors are commonly used to implement properties, methods, and static methods in Python.
# By using metaclasses and descriptors, you can gain a deep understanding of Python's object model and customize class creation and attribute access to suit your needs. This allows for advanced patterns like singletons, registries, and more complex attribute management.
# Example of a simple metaclass
# class MyMeta(type):
#     def __new__(cls, name, bases, attrs):
#         print(f"Creating class {name}")
#         return super().__new__(cls, name, bases, attrs)
# class MyClass(metaclass=MyMeta):
#     pass
# # Output: Creating class MyClass
# Example of a simple descriptor
# class Descriptor:
#     def __get__(self, instance, owner):
#         return instance._value
#     def __set__(self, instance, value):
#         instance._value = value
# class MyClass:
#     attr = Descriptor()
#     def __init__(self, value):
#         self._value = value
# obj = MyClass(10)
# print(obj.attr)  # Output: 10

## Concurrency and Parallelism: Mastering threading, multiprocessing, asyncio, and concurrent.futures for building high-performance applications.
#-->
# Concurrency and parallelism are techniques used to improve the performance of applications by allowing multiple tasks to run simultaneously or overlap in execution.
# Threading is a way to achieve concurrency by creating multiple threads within a single process. Threads share the same memory space, making it easy to share data between them. However, due to the Global Interpreter Lock (GIL) in CPython, only one thread can execute Python bytecode at a time, which can limit performance for CPU-bound tasks. Threading is more suitable for I/O-bound tasks where threads can release the GIL while waiting for I/O operations to complete.
# Multiprocessing is a way to achieve parallelism by creating multiple processes, each with its own memory space. This allows true parallel execution on multiple CPU cores, making it suitable for CPU-bound tasks. The multiprocessing module in Python provides a simple way to create and manage processes.
# asyncio is a library for writing concurrent code using the async/await syntax. It is designed for I/O-bound and high-level structured network code. asyncio uses an event loop to manage and schedule tasks, allowing the program to perform other operations while waiting for I/O operations to complete.
# concurrent.futures is a high-level interface for asynchronously executing callables using threads or processes. It provides the ThreadPoolExecutor and ProcessPoolExecutor classes, which make it easy to manage a pool of threads or processes and submit tasks for execution.
# By mastering these techniques, you can build high-performance applications that efficiently utilize system resources and handle multiple tasks concurrently or in parallel.
# Example of threading
# import threading
# def worker():
#     print("Worker thread")
# thread = threading.Thread(target=worker)
# thread.start()
# thread.join()
# Example of multiprocessing
# from multiprocessing import Process
# def worker():
#     print("Worker process")
# process = Process(target=worker)
# process.start()
# process.join()
# Example of asyncio
# import asyncio
# async def worker():
#     print("Worker coroutine")
# asyncio.run(worker())
# Example of concurrent.futures
# from concurrent.futures import ThreadPoolExecutor
# def worker():
#     print("Worker function")
# with ThreadPoolExecutor(max_workers=2) as executor:
#     executor.submit(worker)

# Dataclasses are a feature in Python that provides a decorator and functions for automatically adding special methods to user-defined classes. Introduced in Python 3.7 via the dataclasses module, they simplify the process of creating classes that primarily store data by automatically generating methods like __init__(), __repr__(), __eq__(), and others based on class attributes.
# To use dataclasses, you decorate a class with @dataclass and define its attributes using type annotations. This reduces boilerplate code and makes the class definition cleaner and more readable.

# Give the best definition and explanation in interview style of a senior engineer:
# As a senior engineer, I would say: "Dataclasses in Python, introduced in version 3.7, provide a decorator and functions for automatically generating special methods such as __init__(), __repr__(), and __eq__() for user-defined classes. This feature significantly reduces boilerplate code when creating classes that are primarily used to store data, making the codebase cleaner, more maintainable, and less error-prone. By leveraging type annotations and the @dataclass decorator, developers can focus on the business logic rather than repetitive method definitions, while still benefiting from immutability, default values, and comparison operations out of the box."


## Define immutability in Python and explain how it can be achieved.#-->
# Immutability in Python refers to the property of an object whose state or value cannot be changed after it is created. Immutable objects cannot be modified, and any operation that seems to modify an immutable object actually creates a new object with the modified value.
# Common immutable types in Python include:
# int
# float
# str
# tuple
# frozenset
# bytes

## Give the best defination and explanation in interview style of a senior engineer 
# concurrency and parallelism:
# As a senior engineer, I would say: "Concurrency and parallelism are essential concepts for building high-performance applications. Concurrency allows multiple tasks to make progress by overlapping their execution, which is particularly useful for I/O-bound operations where tasks can wait for external resources. In Python, we can achieve concurrency using threading and asyncio. Threading is suitable for I/O-bound tasks, while asyncio provides an event-driven approach for managing asynchronous operations efficiently."

### PYTHON CONCURRENCY & HIGH-PERFORMANCE PROGRAMMING

## Manager
# In multiprocessing, a Manager is a server process that holds Python objects and allows other processes to manipulate them using proxies
# Creates shared objects across multiple processes to safely exchange data
# Supports shared lists, dictionaries, namespaces, locks, and other synchronization primitives
# Handles serialization and synchronization automatically
# Example use: `with Manager() as manager: shared_dict = manager.dict(); processes = [Process(target=f, args=(shared_dict,)) for i in range(10)]`
# Trade-off: Slower than direct shared memory but safer and easier to use

## Future
# A placeholder for a result that will be available later (asynchronous operations)
# Core component of concurrent.futures and asyncio
# Key methods: .done(), .result(), .cancel(), .add_done_callback()
# States: pending → running → done/cancelled
# Similar to Promises in JavaScript but with Python-specific implementation
# Example: `future = executor.submit(function, arg1, arg2)`

## Executor
# Abstract base class for executing callables asynchronously
# Two main implementations: ThreadPoolExecutor and ProcessPoolExecutor
# Key methods: .submit() returns a Future, .map() applies function to iterables
# Manages resource allocation, scheduling, and cleanup via context managers
# Example: `with Executor() as executor: results = list(executor.map(func, items))`

## ThreadPoolExecutor
# Pool of worker threads for executing tasks concurrently
# Best for I/O-bound tasks due to Python's GIL
# Constructor param `max_workers` controls thread pool size
# Thread reuse improves performance vs creating threads manually
# Example: `with ThreadPoolExecutor(max_workers=4) as executor: future = executor.submit(fetch_url, url)`

## ProcessPoolExecutor
# Pool of worker processes for executing tasks in parallel
# Best for CPU-bound tasks to bypass GIL limitations
# Spawns separate Python interpreter instances (multiprocessing)
# Serializes function arguments and results (pickle)
# Limited to picklable objects and functions
# Example: `with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor: results = executor.map(process_chunk, data_chunks)`

## Tasks (asyncio)
# High-level wrapper around coroutines in asyncio
# Created via asyncio.create_task() or ensure_future()
# Scheduled for execution on the event loop
# Can be awaited, cancelled, and composed
# Similar to Futures but specifically for coroutines
# Example: `task = asyncio.create_task(coroutine()); await task`

## Event Loop
# Core of asyncio's concurrency model
# Orchestrates execution of async tasks and callbacks
# Monitors I/O operations and schedules tasks when ready
# Single-threaded design with cooperative multitasking
# Methods: run_until_complete(), run_forever(), stop()
# Example: `loop = asyncio.get_event_loop(); loop.run_until_complete(main())`

## Threading vs Multiprocessing vs Asyncio
# Threading:
#   - Uses OS threads within same process and memory space
#   - Limited by GIL (one Python thread at a time)
#   - Good for I/O-bound tasks, easy data sharing
#   - Issues: race conditions, deadlocks, thread overhead
#   - API: threading.Thread, daemon threads, locks, semaphores
#
# Multiprocessing:
#   - Uses separate OS processes with isolated memory
#   - Bypasses GIL for true parallelism
#   - Good for CPU-bound tasks, utilizes multiple cores
#   - Challenges: IPC overhead, serialization, shared memory management
#   - API: Process, Pool, Queue, Pipe, shared memory
#
# Asyncio:
#   - Single-threaded cooperative multitasking via coroutines
#   - Based on event loop and non-blocking I/O
#   - Excellent for I/O-bound tasks with many concurrent operations
#   - No parallelism, requires async-compatible libraries
#   - API: async/await, gather, create_task, run

## concurrent.futures
# High-level interface for asynchronous execution
# Unified API for thread and process pools
# Executor.submit() vs Executor.map() approaches
# wait() and as_completed() for managing multiple futures
# Context managers handle resource cleanup automatically
# Simpler than raw threading/multiprocessing but less flexible
# Example: `results = executor.map(process_item, items)`

## Joblib
# Scientific computing focused parallel execution library
# Optimized for NumPy arrays with shared memory
# Transparent disk-based caching (Memory class)
# Simple interface: Parallel(n_jobs=-1)(delayed(func)(i) for i in items)
# Handles large data efficiently with less overhead than pickle
# Preferred in data science workflows (scikit-learn uses it)

## Celery
# Distributed task queue system for production applications
# Architecture: producer → broker (Redis/RabbitMQ) → worker processes
# Features: scheduling, retries, monitoring, workflows
# Scales across machines, fault-tolerant
# Result backends store task states and returns
# Example: `@app.task def process(x): return x * x; result = process.delay(10)`

## functools: Key Components
# lru_cache: Memoization decorator with LRU eviction policy
#   - @lru_cache(maxsize=128) caches function results
#   - Speeds up repeated calls with same args
#   - Perfect for recursive algorithms, API calls
# reduce: Applies function cumulatively to sequence items
#   - reduce(lambda x, y: x+y, [1,2,3,4]) → 10
#   - Functional programming cornerstone
# partial: Creates new function with pre-filled arguments
#   - base_2 = partial(int, base=2); base_2('1010') → 10
#   - Specializes functions without complex wrappers
# singledispatch: Function overloading based on arg type
#   - Enables polymorphic behavior without class hierarchies
#   - Register type-specific implementations with @func.register(type)

## itertools: Advanced Iteration Utilities
# product: Cartesian product of iterables (nested loops)
#   - product('AB', [1,2]) → A1,A2,B1,B2
# permutations: All possible orderings (no repeated elements)
#   - permutations('ABC', 2) → AB,AC,BA,BC,CA,CB
# combinations: r-length subsequences (order doesn't matter)
#   - combinations('ABCD', 2) → AB,AC,AD,BC,BD,CD
# groupby: Groups consecutive items by key function
#   - Creates sub-iterators for each group
# islice: Lazy slicing for any iterable
#   - islice(range(100), 10, 20) → 10,11,12...19
# chain, cycle, repeat: Iterable composition utilities

## Generator Advanced Features
# send(): Injects values into generator at yield points
#   - Allows two-way communication
#   - gen.send(value) replaces the yield expression with value
# throw(): Raises exception inside generator at yield point
#   - Used for error handling and control flow
#   - gen.throw(Exception) propagates exception at yield
# close(): Terminates generator cleanly
#   - Raises GeneratorExit inside generator
#   - Allows cleanup actions in try/finally blocks
# Example: `def echo(): msg = yield; while True: msg = yield msg`

## Coroutines (async/await)
# Language-level asynchronous programming support (Python 3.5+)
# async def: Defines a coroutine function
# await: Pauses coroutine execution until awaitable completes
# await only usable inside async functions
# Non-blocking by design, cooperatively yields control
# asyncio.gather(): Runs awaitables concurrently
# Example: `async def fetch_data(): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text()`

## requests vs httpx
# requests:
#   - Synchronous HTTP client standard in Python
#   - Simple, intuitive API with broad support
#   - Automatic connection pooling, session cookies
#   - No native async support
# httpx:
#   - Modern HTTP client with both sync and async APIs
#   - Full requests compatibility plus new features
#   - HTTP/2 support, streaming responses
#   - Same API in both sync/async modes
#   - Example: `async with httpx.AsyncClient() as client: r = await client.get(url)`

## collections Module: Specialized Container Types
# deque: Double-ended queue with O(1) append/pop from both ends
#   - Thread-safe, efficient implementation
#   - Fixed size option with automatic discard
#   - Faster than lists for queue operations
# Counter: Dict subclass for counting hashable objects
#   - c = Counter('abracadabra') → {'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1}
#   - .most_common(n), update(), +/- operations
# defaultdict: Dict with default factory for missing keys
#   - dd = defaultdict(list); dd['key'].append(1) # No KeyError
#   - Eliminates key existence checks
# OrderedDict: Dict that remembers insertion order
#   - Maintains key ordering even with modifications
#   - Special operations: .popitem(last=False), move_to_end()
# ChainMap: Multiple dicts viewed as single mapping
#   - Searches through chain of dicts for keys
#   - Perfect for layered configs, scopes
# namedtuple: Tuple with named fields
#   - Point = namedtuple('Point', ['x', 'y'])
#   - Lightweight, immutable object alternative

## Priority Queue & Binary Search Utilities
# heapq: Binary heap queue algorithm implementation
#   - heappush(heap, item), heappop(heap)
#   - O(log n) push/pop operations
#   - min-heap by default (smallest item first)
#   - nlargest(), nsmallest(), heapify()
#   - Used for priority queues, scheduling
# bisect: Binary search in sorted sequences
#   - bisect_left(), bisect_right() find insertion points
#   - insort_left(), insort_right() insert while maintaining order
#   - O(log n) search in sorted lists
# queue: Thread-safe queue implementations
#   - Queue: FIFO queue
#   - LifoQueue: LIFO queue (stack)
#   - PriorityQueue: Min value first (heapq-based)
#   - All support blocking operations with timeouts

## Method Resolution Order (MRO)
# Algorithm determining method lookup order in multiple inheritance
# C3 linearization algorithm (Python 2.3+)
# Ensures "good" properties: monotonicity, local precedence
# Access via Class.__mro__ or Class.mro()
# Diamond problem resolution: child → left parent → right parent → base
# Example: `class D(B, C): pass; print(D.__mro__)`
# Key to understanding super() behavior in complex inheritance

## Metaclasses
# "Classes of classes" - factories that build classes
# Control class creation, initialization, behavior
# type: Built-in metaclass that creates all classes
#   - type('Name', (bases,), {attributes})
# Custom metaclasses inherit from type
# __new__, __init__, __prepare__ key methods to override
# Use cases: 
#   - Registry patterns (Django models)
#   - API enforcement (abstract methods)
#   - Auto-decorating methods
#   - Class-level aspects/hooks
# Example: `class Meta(type): def __new__(cls, name, bases, attrs): print(f"Creating {name}"); return super().__new__(cls, name, bases, attrs)`

### OBJECT-ORIENTED PROGRAMMING (OOP) define oops in the best detailed form, and classes and objects full definations, like how a senior software engineer will explain in an interview.
# As a senior engineer, I'd define Object-Oriented Programming (OOP) as a programming paradigm centered around the concept of "objects." These objects are self-contained units that bundle together data (attributes) and the methods (functions) that operate on that data. The core idea is to model real-world entities and their interactions in a way that makes code more modular, reusable, and easier to manage, especially in large, complex systems.

# **Classes and Objects**
# A **Class** is the blueprint for creating objects. It defines a set of attributes and methods that the created objects will have. For example, a `Car` class would define attributes like `color` and `max_speed`, and methods like `start_engine()` and `accelerate()`. It's a logical template, not a physical entity in memory.

# An **Object** is a specific instance of a class. If `Car` is the blueprint, then a red Ferrari or a blue Tesla are objects created from that blueprint. Each object has its own state (the values of its attributes) but shares the same behavior (the methods defined in the class).

# The power of OOP comes from its four fundamental principles:

## Core OOP Principles
# Abstraction: Hiding implementation details, exposing only relevant interfaces
#   - Defines what an object does without specifying how
#   - Python example: Abstract Base Classes (ABC module)
#
# Encapsulation: Bundling data and methods, controlling access
#   - Name mangling with double underscore (__var)
#   - Properties (@property) for controlled access
#   - Information hiding through conventions (_var)
#
# Inheritance: Creating new classes from existing ones
#   - Reuse code, establish "is-a" relationships
#   - Multiple inheritance with MRO for resolution
#   - Types: single, multiple, multilevel, hierarchical
#
# Polymorphism: Objects behaving differently based on context
#   - Duck typing: "If it walks like a duck and quacks like a duck..."
#   - Method overriding: redefining methods from parent class
#   - Operator overloading via dunder methods

## Inheritance Types
# Single Inheritance: One parent class only
#   - Simple, direct "is-a" relationship
#   - Example: `class Dog(Animal): pass`
#
# Multiple Inheritance: Multiple parent classes
#   - Combines behaviors from multiple sources
#   - Complexity with method resolution order (MRO)
#   - Example: `class Bat(Mammal, FlyingAnimal): pass`
#
# Multilevel Inheritance: Chain of inheritance
#   - Forms parent → child → grandchild relationship
#   - Example: `class Wolf(Canine): pass; class Dog(Wolf): pass`
#
# Hierarchical Inheritance: Multiple children of one parent
#   - Common base class specialization
#   - Example: `class Dog(Animal): pass; class Cat(Animal): pass`

## Abstract Classes & Interfaces
# Abstract classes (ABC module):
#   - Cannot be instantiated directly
#   - Define interface via @abstractmethod
#   - May include concrete methods (mixins)
#   - Example: `from abc import ABC, abstractmethod; class Shape(ABC): @abstractmethod def area(self): pass`
#
# Interfaces (informal in Python):
#   - Pure abstract classes (only abstract methods)
#   - Define contracts for implementing classes
#   - Multiple interfaces can be implemented
#   - Protocol classes in typing module (structural typing)

## Composition vs Aggregation
# Composition: Strong "has-a" relationship with lifecycle binding
#   - Container owns component, component doesn't exist independently
#   - Component created/destroyed with container
#   - Example: `class Car: def __init__(self): self.engine = Engine()`
#
# Aggregation: Weak "has-a" relationship without lifecycle binding
#   - Container uses component, component exists independently
#   - Example: `class University: def __init__(self, departments): self.departments = departments`

## SOLID Principles
# Single Responsibility: A class should have only one reason to change
#   - Each class handles one specific concern
#   - Example: Separate data access, business logic, UI classes
#
# Open/Closed: Open for extension, closed for modification
#   - Extend behavior without altering existing code
#   - Use inheritance, strategy pattern, decorators
#
# Liskov Substitution: Subtypes must be substitutable for their base types
#   - Child classes must fulfill parent's contracts
#   - No unexpected behaviors when using subclass instances
#
# Interface Segregation: Clients shouldn't depend on interfaces they don't use
#   - Small, focused interfaces over monolithic ones
#   - Protocol classes, mixins implement specific behaviors
#
# Dependency Inversion: Depend on abstractions, not concretions
#   - High-level modules independent of low-level implementations
#   - Dependency injection, abstract classes, function parameters

### OBSERVABILITY IN PYTHON APPLICATIONS

## Telemetry, Logging, Monitoring & Tracing
# Logging: Recording events for later analysis
#   - Python's logging module: hierarchical loggers, handlers, formatters
#   - Structured logging (JSON) with python-json-logger
#   - Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
#   - Example: `logger.info("User %s logged in", user_id, extra={"ip": ip_addr})`
#
# Monitoring: Real-time system health observation
#   - Metrics collection: counters, gauges, histograms
#   - Prometheus client, statsd, datadog libraries
#   - Example: `REQUEST_COUNT.inc(); with RESPONSE_TIME.time(): process_request()`
#
# Tracing: Following request flow across distributed systems
#   - OpenTelemetry, OpenTracing standards
#   - Spans, contexts, propagation
#   - Visualized in tools like Jaeger, Zipkin
#
# Telemetry: Holistic approach combining all above
#   - OpenTelemetry as unified standard
#   - Correlating logs, metrics, traces
#   - Context propagation across service boundaries

### CPYTHON INTERNALS & AI WORKLOAD CONSIDERATIONS

## CPython Internals
# Bytecode: Intermediate representation between Python and machine code
#   - Generated by CPython's compiler
#   - View with dis module: `import dis; dis.dis(function)`
#   - Consists of opcodes and operands
#   - Stored in .pyc files for faster startup
#
# Python Virtual Machine (PVM):
#   - Stack-based interpreter executing bytecode
#   - Manages execution context, memory, GIL
#   - Implements Python's runtime behavior

## Concurrency Choice for AI Workloads
# Multiprocessing:
#   - Best for CPU-bound computations: model training, batch inference
#   - Utilizes multiple cores, bypasses GIL
#   - Data parallelism via shared memory (numpy arrays)
#   - Challenges: process startup overhead, memory usage
#
# Threading:
#   - Limited value for pure computation (GIL)
#   - Useful for I/O operations during AI workflows
#   - Managing multiple concurrent data sources
#   - Good when using C extension libraries that release GIL
#
# Asyncio:
#   - Excellent for API serving, distributed model deployment
#   - High-concurrency, non-blocking request handling
#   - Integrates with frameworks like FastAPI
#   - Best for serving models rather than training

### DESIGN PATTERNS IN ML/AI

## Factory Pattern
# Provides interface for creating objects without specifying concrete classes
# In ML: Model selection and configuration
# Example: ModelFactory.create("regression", params) returns appropriate model
# Benefits: Centralizes model creation logic, simplifies switching algorithms

## Strategy Pattern
# Defines family of algorithms, encapsulates each, makes them interchangeable
# In ML: Optimization algorithms (SGD, Adam, RMSprop)
# Example: model.compile(optimizer=Adam(lr=0.001))
# Benefits: Swap optimization strategies without changing model code

## Singleton Pattern
# Ensures class has only one instance with global access point
# In ML: Configuration management, resource allocation
# Example: GPU memory managers, experiment tracking clients
# Benefits: Controls access to scarce resources, ensures consistent config

## Observer Pattern
# Defines one-to-many dependency between objects
# In ML: Model training callbacks, logging, monitoring
# Example: TensorFlow/PyTorch callbacks for training events
# Benefits: Decouple monitoring from training logic

## Custom __call__ Classes
# Makes instances callable like functions
# In ML: PyTorch nn.Module, TensorFlow layers
# Example: `layer = Dense(128); output = layer(input)`
# Benefits: Object-oriented structure with functional usage
# Enables composition of transformations in elegant pipelines



# Object-Oriented Programming is a paradigm centered around "objects" which bundle data (attributes) and the code that operates on that data (methods). These core principles are not just theoretical rules; they are powerful tools for building software that is modular, flexible, and easy to maintain.

# 1. Abstraction
# High-Level Concept: Abstraction is the principle of hiding complex implementation details and exposing only the essential, high-level functionality. It's about defining what an object can do, not how it does it. Think of a car's dashboard: you have a steering wheel, pedals, and a gear stick. You don't need to know about the combustion engine's mechanics or the transmission's inner workings to drive the car.

# Python Implementation: In Python, abstraction is most formally achieved using the abc (Abstract Base Classes) module.

# An Abstract Base Class (ABC) acts as a blueprint. It cannot be instantiated on its own.
# The @abstractmethod decorator is used to define methods that must be implemented by any concrete (i.e., non-abstract) subclass.
# This enforces a contract. If a class inherits from Shape, it must provide an implementation for the area method, guaranteeing a consistent interface across all shapes.

# from abc import ABC, abstractmethod

# # Abstract class defining the contract for all shapes
# class Shape(ABC):
#     @abstractmethod
#     def area(self):
#         """Return the area of the shape."""
#         pass

#     @abstractmethod
#     def perimeter(self):
#         """Return the perimeter of the shape."""
#         pass

# # Concrete class implementing the contract
# class Square(Shape):
#     def __init__(self, side_length):
#         self.side_length = side_length

#     def area(self):
#         return self.side_length ** 2

#     def perimeter(self):
#         return 4 * self.side_length

# # This would raise a TypeError because Shape has abstract methods
# # invalid_shape = Shape()

# # This is valid because Square implements all abstract methods
# my_square = Square(5)
# print(f"Area: {my_square.area()}") # Output: Area: 25

# 2. Encapsulation
# High-Level Concept: Encapsulation is the practice of bundling an object's data (attributes) and methods together within a class, and controlling access to that data. The goal is to protect the object's internal state from outside interference and misuse, ensuring data integrity.

# Python Implementation: Python doesn't have private or public keywords like Java or C++. Instead, it uses conventions and a feature called name mangling.

# Protected (by convention): A single leading underscore (_) prefixes an attribute (e.g., _internal_var). This is a signal to other developers: "This is for internal use. Don't touch it directly unless you know what you're doing." The interpreter does not enforce this.
# Private (via Name Mangling): A double leading underscore (__) prefixes an attribute (e.g., __private_var). This triggers name mangling, where Python renames the attribute internally to _ClassName__private_var. This is not true privacy but is effective at preventing accidental overriding in subclasses.
# Properties (The Pythonic Way): The @property decorator provides the best of both worlds. It allows you to expose an attribute publicly while using getter, setter, and deleter methods to control access. This lets you add validation or computation logic without changing the class's public interface.

# class Account:
#     def __init__(self, initial_balance):
#         # Protected attribute by convention
#         self._transaction_log = []
#         # Private attribute via name mangling
#         self.__balance = initial_balance

#     @property
#     def balance(self):
#         """Getter for the balance. Publicly accessible as 'account.balance'."""
#         return self.__balance

#     @balance.setter
#     def balance(self, new_balance):
#         """Setter for the balance. Allows for validation."""
#         if new_balance < 0:
#             raise ValueError("Balance cannot be negative.")
#         self.__balance = new_balance

# my_account = Account(100)

# # Accessing the balance via the property
# print(f"Current balance: {my_account.balance}") # Output: Current balance: 100

# # Setting the balance via the property setter
# my_account.balance = 150
# print(f"New balance: {my_account.balance}") # Output: New balance: 150

# # This will raise an AttributeError because of name mangling
# # print(my_account.__balance)

# # This will raise a ValueError due to our validation logic
# # my_account.balance = -50

# 3. Inheritance
# High-Level Concept: Inheritance allows a new class (the child or subclass) to acquire, or "inherit," the attributes and methods of an existing class (the parent or superclass). This fosters an "is-a" relationship (e.g., a Dog is an Animal) and is a cornerstone of code reuse.

# Python Implementation: You define inheritance by passing the parent class into the child class definition.

# Single Inheritance: A class inherits from one parent. This is the most common and straightforward type.
# Multiple Inheritance: A class inherits from multiple parents. Python handles this using a deterministic algorithm called the Method Resolution Order (MRO) to decide which parent's method to use if there's a name conflict. You can inspect the MRO using ClassName.mro(). While powerful, it can lead to complex and hard-to-debug code (the "Diamond Problem"), so it should be used judiciously. Mixin classes are a common and effective use case.
# class Animal:
#     def __init__(self, name):
#         self.name = name

#     def speak(self):
#         raise NotImplementedError("Subclass must implement this method")

# # Single Inheritance: Dog "is-a" Animal
# class Dog(Animal):
#     def speak(self):
#         return f"{self.name} says Woof!"

# # Multiple Inheritance: A Bat "is-a" Mammal and "is-a" FlyingCreature
# class Mammal:
#     def feed_milk(self):
#         return "Feeding milk."

# class FlyingCreature:
#     def fly(self):
#         return "Taking flight."

# class Bat(Mammal, FlyingCreature):
#     pass

# my_dog = Dog("Rex")
# print(my_dog.speak()) # Output: Rex says Woof!

# my_bat = Bat()
# print(my_bat.feed_milk()) # Output: Feeding milk.
# print(my_bat.fly())      # Output: Taking flight.

# # You can see the Method Resolution Order
# print(Bat.mro())
# # Output: [<class '__main__.Bat'>, <class '__main__.Mammal'>, <class '__main__.FlyingCreature'>, <class 'object'>]

# 4. Polymorphism
# High-Level Concept: Polymorphism, from the Greek for "many forms," is the ability of an object to take on many forms. In practice, it means you can call the same method on different objects, and each object will respond in its own way.

# Python Implementation: Python's dynamic typing makes polymorphism natural. This is often called "duck typing": if it walks like a duck and quacks like a duck, then it must be a duck. Python doesn't care about an object's type, only that it has the method or attribute you're trying to use.

# Method Overriding: A subclass can provide its own specific implementation of a method that is already defined in its parent class. This is polymorphism in the context of inheritance.
# Operator Overloading: You can define how standard operators like +, *, and len() work with your custom objects by implementing special "dunder" (double underscore) methods (e.g., __add__, __mul__, __len__).
# class Cat:
#     def speak(self):
#         return "Meow"

# class Dog:
#     def speak(self):
#         return "Woof"

# class Car:
#     def speak(self):
#         return "Vroom"

# # Polymorphism in action: the `animal_sound` function doesn't care
# # about the type of `animal`, only that it has a `speak` method.
# def animal_sound(animal):
#     print(animal.speak())

# animal_sound(Cat()) # Output: Meow
# animal_sound(Dog()) # Output: Woof
# animal_sound(Car()) # Output: Vroom (This works because of duck typing!)

# Composition vs. Aggregation
# These are two ways to model "has-a" relationships, where one object is composed of or contains other objects. They are alternatives to inheritance ("is-a"). A common design principle is to "favor composition over inheritance" as it often leads to more flexible and less coupled designs.

# Composition (Strong "has-a"): The component object's lifecycle is tied to the container object's lifecycle. If the container is destroyed, the component is also destroyed. The component cannot exist independently.

# Example: A Car has an Engine. The Engine is created when the Car is created and is destroyed with the Car. The Engine doesn't make sense outside the context of a Car.
# Aggregation (Weak "has-a"): The component object can exist independently of the container. The container "has" a reference to the component, but doesn't own it.

# Example: A Classroom has Students. The Students exist before the Classroom is formed and continue to exist after the Classroom is disbanded.

# Composition: The Engine is part of the Car
# class Engine:
#     def start(self):
#         return "Engine starting..."

# class Car:
#     def __init__(self):
#         # The Engine is created and owned by the Car instance
#         self.engine = Engine()

#     def drive(self):
#         print(self.engine.start())
#         print("Car is moving.")

# # When `my_car` is destroyed, its `Engine` instance is also gone.
# my_car = Car()
# my_car.drive()


# # Aggregation: The Professor exists independently of the Department
# class Professor:
#     def __init__(self, name):
#         self.name = name

# class Department:
#     def __init__(self, name, professors):
#         # The Department is given a list of existing Professor objects
#         self.name = name
#         self.professors = professors

# # Professors are created independently
# prof_einstein = Professor("Albert Einstein")
# prof_curie = Professor("Marie Curie")

# # The Department aggregates these independent objects
# physics_dept = Department("Physics", [prof_einstein, prof_curie])

# # If `physics_dept` is deleted, `prof_einstein` and `prof_curie` still exist.

# Constructors
# In Python, the constructor method is defined using __init__. It is called automatically when a new instance of a class is created. The primary purpose of the constructor is to initialize the object's attributes and set up any necessary state.
# class Person:
#     def __init__(self, name, age):
#         self.name = name
#         self.age = age
# # Example usage
# person = Person("Alice", 30)
# print(person.name)  # Output: Alice
# print(person.age)   # Output: 30  
# In this example, the __init__ method initializes the name and age attributes of the Person object when it is created.
# Immutable Objects
# An immutable object is an object whose state or value cannot be modified after it has been created. In Python, immutable objects include types like int, float, str, tuple, and frozenset. When you attempt to change an immutable object, a new object is created instead of modifying the existing one.

# Constructor Overloading, operator overloading, 
# In Python, constructor overloading is not supported in the traditional sense as seen in languages like Java or C++. However, you can achieve similar functionality using default arguments or variable-length arguments (*args and **kwargs) in the __init__ method.
# class Rectangle:
#     def __init__(self, width, height):
#         self.width = width
#         self.height = height  
#     def area(self):
#         return self.width * self.height
# # Example usage
# rect1 = Rectangle(10, 20)
# print(rect1.area())  # Output: 200
# rect2 = Rectangle(15, 25)
# print(rect2.area())  # Output: 375
# In this example, the Rectangle class has a single constructor that takes width and height as parameters. You can create multiple instances of Rectangle with different dimensions.
# Operator overloading allows you to define custom behavior for standard operators (like +, -, *, etc.) when they are used with instances of your classes. This is done by defining special methods (also known as "dunder" methods) in your class.

# Abstract classes and Interfaces each and every point with examples
# Abstract Classes
# An abstract class is a class that cannot be instantiated on its own and is meant to be subclassed. It can contain abstract methods (methods without implementation) that must be implemented by any concrete subclass. In Python, you can create abstract classes using the abc module.
# from abc import ABC, abstractmethod
# # Abstract class defining the contract for all shapes
# class Shape(ABC):
#     @abstractmethod
#     def area(self):
#         """Return the area of the shape."""
#         pass

#     @abstractmethod
#     def perimeter(self):
#         """Return the perimeter of the shape."""
#         pass
# # Concrete class implementing the contract
# class Square(Shape):
#     def __init__(self, side_length):
#         self.side_length = side_length

#     def area(self):
#         return self.side_length ** 2

#     def perimeter(self):
#         return 4 * self.side_length
# # This would raise a TypeError because Shape has abstract methods
# # invalid_shape = Shape()
# # This is valid because Square implements all abstract methods
# my_square = Square(5)
# print(f"Area: {my_square.area()}") # Output: Area: 25
# print(f"Perimeter: {my_square.perimeter()}") # Output: Perimeter: 20
# # Interfaces
# # In Python, interfaces are typically implemented using abstract base classes (ABCs) with only abstract methods. An interface defines a contract that implementing classes must follow, ensuring they provide specific functionality.
# # Example of an interface using ABC
# class Drawable(ABC):
#     @abstractmethod
#     def draw(self):
#         """Draw the object."""
#         pass
# class Resizable(ABC):
#     @abstractmethod
#     def resize(self, scale_factor):
#         """Resize the object by a scale factor."""
#         pass
# # Concrete class implementing multiple interfaces
# class Circle(Drawable, Resizable):
#     def __init__(self, radius):
#         self.radius = radius

#     def draw(self):
#         return f"Drawing a circle with radius {self.radius}"

#     def resize(self, scale_factor):
#         self.radius *= scale_factor
#         return f"Resized circle to radius {self.radius}"
    
# # Example usage
# my_circle = Circle(10)
# print(my_circle.draw())  # Output: Drawing a circle with radius 10
# print(my_circle.resize(1.5))  # Output: Resized circle to radius 15.0
# # In this example, Drawable and Resizable act as interfaces. The Circle class implements both interfaces by providing concrete implementations for the draw and resize methods.

# Abstract Classes
# Cannot be instantiated directly
# Define interface via @abstractmethod
# May include concrete methods (mixins)
# class Drawable(ABC):
#     @abstractmethod
#     def draw(self):
#         pass
# class Resizable(ABC):
#     @abstractmethod
#     def resize(self, scale_factor):
#         pass
# # Concrete class implementing multiple interfaces
# class Circle(Drawable, Resizable):
#     def __init__(self, radius):
#         self.radius = radius
#     def draw(self):
#         return f"Drawing a circle with radius {self.radius}"
#     def resize(self, scale_factor):
#         self.radius *= scale_factor
#         return f"Resized circle to radius {self.radius}"
# Interfaces (informal in Python)
# Pure abstract classes (only abstract methods)
# Define contracts for implementing classes
# Multiple interfaces can be implemented
# Protocol classes in typing module (structural typing)
# class Drawable(ABC):
#     @abstractmethod
#     def draw(self):
#         pass
# class Resizable(ABC):
#     @abstractmethod
#     def resize(self, scale_factor):
#         pass



#### Cover Retrival Augemented Generation (RAG) in detail with examples and use cases. and all the points and interview questions and answers.
# Retrival Augmented Generation (RAG) is an advanced technique in natural language processing (NLP) that combines the strengths of retrieval-based models and generative models to produce more accurate and contextually relevant responses. RAG is particularly useful in scenarios where the model needs to generate text based on a large corpus of documents or knowledge bases.
# Key Components of RAG:
# Retrieval Model: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query. Common retrieval models include dense vector search (using embeddings) and traditional keyword-based search (like TF-IDF or BM25).
# Generative Model: This component generates text based on the retrieved documents. It typically uses transformer-based architectures like GPT, BERT, or T5 to produce coherent and contextually appropriate responses.
# Integration Mechanism: RAG integrates the retrieval and generative components. The retrieved documents are used as additional context for the generative model, allowing it to produce responses that are informed by the most relevant information available.
# How RAG Works:
# Input Query: The user provides a query or prompt.
# Document Retrieval: The retrieval model searches the corpus and returns a set of relevant documents or passages.
# Contextual Generation: The generative model takes the input query and the retrieved documents as context
# to generate a response.
# Output Response: The final output is a generated response that is both contextually relevant and informative.
# Example Use Case:
# Let's say we have a large corpus of articles about various programming languages. A user asks, "What are the key features of Python?" The RAG model would:
# Use the retrieval model to find articles or passages related to Python.
# Pass the retrieved content along with the user's query to the generative model.
# Generate a response that highlights Python's key features based on the retrieved information.
# Example Implementation:
# from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration
# import torch
# # Initialize tokenizer, retriever, and model
# tokenizer = RagTokenizer.from_pretrained("facebook/rag-sequence-nq")
# retriever = RagRetriever.from_pretrained("facebook/rag-sequence-nq")
# model = RagSequenceForGeneration.from_pretrained("facebook/rag-sequence-nq")
# # Input query
# query = "What are the key features of Python?"
# # Tokenize input
# inputs = tokenizer([query], return_tensors="pt")
# # Retrieve relevant documents
# input_ids = inputs["input_ids"]
# retrieved_docs = retriever(input_ids.numpy(), return_tensors="pt")
# # Generate response
# outputs = model.generate(input_ids=input_ids, context_input_ids=retrieved_docs["context_input_ids"], context_attention_mask=retrieved_docs["context_attention_mask"])
# # Decode and print the response
# response = tokenizer.batch_decode(outputs, skip_special_tokens=True)
# print(response)
# Output: ['Python is a high-level, interpreted programming language known for its readability and simplicity. Key features include dynamic typing, automatic memory management, and a vast standard library.']
# Advantages of RAG:
# Enhanced Contextual Understanding: By leveraging retrieved documents, RAG can generate responses that are more informed and contextually relevant.
# Improved Accuracy: The combination of retrieval and generation helps reduce hallucinations (incorrect or fabricated information
# often seen in pure generative models).
# Flexibility: RAG can be adapted to various domains by changing the underlying corpus used for retrieval.
# Scalability: It can handle large knowledge bases efficiently by focusing on relevant information.
# Challenges and Considerations:
# Computational Complexity: RAG models can be resource-intensive due to the dual nature of retrieval and generation.

## Transformers & Attention Mechanism in detail with examples and use cases. and all the points and interview questions and answers.
# Transformers are a type of deep learning architecture that has revolutionized the field of natural language processing (NLP) and beyond. Introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017, transformers leverage self-attention mechanisms to process input data in parallel, enabling them to handle long-range dependencies more effectively than previous architectures like RNNs and LSTMs.
# Key Components of Transformers:
# Self-Attention Mechanism: This is the core innovation of transformers. It allows the model to weigh the importance of different words in a sentence relative to each other, enabling it to capture context and relationships effectively.
# Multi-Head Attention: This extends the self-attention mechanism by allowing the model to focus on different parts of the input simultaneously. Each "head" learns to attend to different aspects of the input.
# Positional Encoding: Since transformers do not have a built-in sense of word order (unlike RNNs), positional encodings are added to the input embeddings to provide information about the position of each word in the sequence.
# Feed-Forward Neural Networks: Each transformer layer includes a fully connected feed-forward network that processes the output of the attention mechanism.
# Layer Normalization and Residual Connections: These techniques help stabilize training and allow for deeper networks by mitigating issues like vanishing gradients.
# Transformer Architecture:
# The transformer architecture consists of an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output sequence. Each consists of multiple layers of self-attention and feed-forward networks.
# Encoder Layer:
# Input Embeddings + Positional Encoding
# Multi-Head Self-Attention
# Feed-Forward Neural Network
# Decoder Layer:
# Input Embeddings + Positional Encoding
# Masked Multi-Head Self-Attention (to prevent attending to future tokens)
# Multi-Head Attention over Encoder Output
# Feed-Forward Neural Network
# Example Use Case:
# Let's consider a simple example of using a transformer model for text classification using the Hugging Face Transformers library.

# BERT (Bidirectional Encoder Representations from Transformers) is a popular transformer-based model for various NLP tasks, including text classification.

# Generative AI models like GPT (Generative Pre-trained Transformer) are based on the transformer architecture and are used for tasks such as text generation, translation, and summarization.
# GANs (Generative Adversarial Networks) are another class of generative models that use a different architecture involving two neural networks (a generator and a discriminator) competing against each other to produce realistic data samples.

# Regression vs Classification
# Regression and classification are two fundamental types of supervised learning tasks in machine learning, each serving different purposes based on the nature of the target variable. 
# Regression:
# Regression is used when the target variable is continuous and numerical. The goal of regression is to predict a value based on input features. Common regression algorithms include Linear Regression, Polynomial Regression, and Support Vector Regression.
# Example Use Case: Predicting house prices based on features like size, location, and number of bedrooms.

# Classification:
# Classification is used when the target variable is categorical. The goal of classification is to assign input data to one of several predefined classes or categories. Common classification algorithms include Logistic Regression, Decision Trees, Random Forests, and Support Vector Machines.
# Example Use Case: Email spam detection, where emails are classified as "spam" or "not spam."

# Linear vs Logistic Regression
# Linear Regression: Linear regression is a regression algorithm used to predict a continuous target variable based on one or more input features. It assumes a linear relationship between the input features and the target variable. The model is trained by minimizing the mean squared error between the predicted and actual values.  
# Example: Predicting house prices based on features like size and location.
# Logistic Regression: Logistic regression is a classification algorithm used to predict the probability of a binary outcome (two classes). It models the relationship between input features and the log-odds of the target variable using the logistic function. The output is a probability value between 0 and 1, which can be thresholded to assign class labels.
# Example: Predicting whether an email is spam or not based on its content.

# Multi Agent System, Ai Model optimization techniques, Hyperparameter tuning techniques
# Multi-Agent System (MAS): A multi-agent system is a system composed of multiple interacting intelligent agents. These agents can be software programs or physical robots that work together to solve complex problems. MAS can be used in various applications, including robotics, distributed control systems, and simulation of social behaviors.
# Example Use Case: Autonomous vehicles coordinating to navigate traffic.
# AI Model Optimization Techniques: AI model optimization techniques are strategies used to improve the performance, efficiency, and effectiveness of machine learning models. These techniques can be applied during various stages of the model development process, including data preprocessing, model training, and inference.
# Common optimization techniques include:
# Hyperparameter Tuning: Adjusting the hyperparameters of a model to find the optimal configuration that maximizes performance.
# Regularization: Techniques like L1 and L2 regularization to prevent overfitting.
# Pruning: Removing unnecessary parts of a model to reduce complexity and improve efficiency.
# Quantization: Reducing the precision of model weights to decrease memory usage and speed up inference.
# Knowledge Distillation: Training a smaller model (student) to mimic the behavior of a larger model (teacher).
# Hyperparameter Tuning Techniques: Hyperparameter tuning is the process of finding the best set of hyperparameters for a machine learning model to optimize its performance. Hyperparameters are parameters that are not learned from the data but are set before training the model. Common hyperparameters include learning rate, batch size, number of layers, and regularization strength.
# Common hyperparameter tuning techniques include:
# Grid Search: Exhaustively searching through a predefined set of hyperparameter values.
# Random Search: Randomly sampling hyperparameter values from a specified distribution.
# Bayesian Optimization: Using probabilistic models to guide the search for optimal hyperparameters.
# Gradient-Based Optimization: Using gradient information to optimize hyperparameters.

# iNFERENCE OPTIMIZATION TECHNIQUES
# Inference optimization techniques are strategies used to improve the speed and efficiency of machine learning models during the inference phase (i.e., when making predictions on new data). These techniques are particularly important for deploying models in production environments where low latency and high throughput are critical.
# Common inference optimization techniques include: 
# Model Compression: Reducing the size of the model through techniques like pruning, quantization, and knowledge distillation.
# Hardware Acceleration: Leveraging specialized hardware (e.g., GPUs, TPUs, FPGAs) to speed up inference.
# Batch Processing: Processing multiple inputs simultaneously to take advantage of parallelism.
# Caching: Storing frequently used predictions to avoid redundant computations.

# Dictionary vs HashMap
# In Python, a dictionary is a built-in data structure that implements a hash table. It allows you to store key-value pairs, where each key is unique and maps to a specific value. Dictionaries provide average O(1) time complexity for lookups, insertions, and deletions due to their underlying hash table implementation.
# Example:
# my_dict = {"apple": 1, "banana": 2, "cherry": 3}
# print(my_dict["banana"])  # Output: 2
# In contrast, a HashMap is a term commonly used in other programming languages (like Java) to refer to a similar data structure that also implements a hash table. While the concepts are similar, the specific implementations and features may vary between languages.
# In summary, a Python dictionary is equivalent to a HashMap in other languages, both serving as key-value stores with efficient access times.
# Bytecode in Python:
# Bytecode is an intermediate representation of your Python code that is generated by the Python interpreter. It is a low-level set of instructions that is executed by the Python Virtual Machine (PVM). Bytecode is platform-independent, meaning it can run on any system that has a compatible PVM.
# When you run a Python script, the interpreter first compiles the source code into bytecode, which is then executed. This compilation step allows Python to be an interpreted language while still benefiting from some performance optimizations.
# You can view the bytecode of a Python function using the built-in dis module:


# hashmap vs concurrenthashmap# In Java, a HashMap is a non-synchronized collection that allows you to store key-value pairs. It is not thread-safe, meaning that if multiple threads access a HashMap concurrently and at least one of the threads modifies the map structurally, it must be synchronized externally.
# Example of HashMap:
# import java.util.HashMap;     
# public class HashMapExample {     
#     public static void main(String[] args) {
#         HashMap<Integer, String> map = new HashMap<Integer, String>();     
#         map.put(1, "One");
#         map.put(2, "Two");
#         System.out.println(map);
#     }
# }
# Output: {1=One, 2=Two}
# On the other hand, a ConcurrentHashMap is a thread-safe variant of HashMap that allows concurrent access by multiple threads without the need for external synchronization. It achieves this by dividing the map into segments and locking only the segment being accessed, allowing for higher concurrency and better performance in multi-threaded environments.
# Example of ConcurrentHashMap:
# import java.util.concurrent.ConcurrentHashMap;
# public class ConcurrentHashMapExample {
#     public static void main(String[] args) {
#         ConcurrentHashMap<Integer, String> map = new ConcurrentHashMap<Integer, String>();
#         map.put(1, "One");
#         map.put(2, "Two");
#         System.out.println(map);
#     }
# }

# like in java we have collection framework in python we have collection module
# The collections module in Python provides alternatives to built-in types that can be more efficient or provide additional functionality. It includes several specialized container datatypes that can be used to store and manipulate data in various ways.
# Some of the most commonly used classes in the collections module include:
# namedtuple(): Factory function for creating tuple subclasses with named fields.
# deque: A double-ended queue that allows you to append and pop elements from both ends with O(1) time complexity.
# Counter: A subclass of dict for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values.
# OrderedDict: A dictionary subclass that maintains the order of keys based on the order of insertion.
# defaultdict: A subclass of dict that provides a default value for a nonexistent key, avoiding KeyError.
# Example usage of collections module:

# Data strctures like linear and non linear data structures, array stack queue linkedlist tree graph, heap, trie, hashtable, how to implement them in python?
# Linear Data Structures:
# Array: An array is a collection of elements identified by index or key. In Python, you can use lists to implement arrays.
# Example:
my_array = [1, 2, 3, 4, 5]
print(my_array[0])  # Output: 1
# Stack: A stack is a linear data structure that follows the Last In First Out (LIFO) principle. You can use lists to implement stacks in Python.
# Example:
my_stack = []
my_stack.append(1)
my_stack.append(2)
print(my_stack.pop())  # Output: 2
print(my_stack)  # Output: [1]
# Queue: A queue is a linear data structure that follows the First In First Out (FIFO) principle. You can use the collections.deque class to implement queues in Python.
# Example:
from collections import deque
my_queue = deque()
my_queue.append(1)
my_queue.append(2)
print(my_queue.popleft())  # Output: 1
print(my_queue)  # Output: deque([2])
# Linked List: A linked list is a linear data structure where each element (node) points to the next node. You can implement a linked list using classes in Python.
# Example:
class Node:
    def __init__(self, data):
        self.data = data
        self.next = None
class LinkedList:
    def __init__(self):
        self.head = None
    def append(self, data):
        new_node = Node(data)
        if not self.head:
            self.head = new_node
            return
        last = self.head
        while last.next:
            last = last.next
        last.next = new_node
    def print_list(self):
        current = self.head
        while current:
            print(current.data, end=" -> ")
            current = current.next
        print("None")
my_linked_list = LinkedList()
my_linked_list.append(1)
my_linked_list.append(2)
my_linked_list.print_list()  # Output: 1 -> 2 -> None
# Non-Linear Data Structures:
# Tree: A tree is a hierarchical data structure where each node has a value and a list of child nodes. You can implement a tree using classes in Python.
# Example:
class TreeNode:
    def __init__(self, value):
        self.value = value
        self.children = []
    def add_child(self, child_node):
        self.children.append(child_node)
    def print_tree(self, level=0):
        print(" " * level + str(self.value))
        for child in self.children:
            child.print_tree(level + 1)
root = TreeNode(1)
child1 = TreeNode(2)
child2 = TreeNode(3)
root.add_child(child1)
root.add_child(child2)
root.print_tree()
# Output:
# 1
#  2

#  3
# Graph: A graph is a collection of nodes (vertices) connected by edges. You can implement a graph using dictionaries or classes in Python.
# Example:
class Graph:
    def __init__(self):
        self.adjacency_list = {}
    def add_edge(self, u, v):
        if u not in self.adjacency_list:
            self.adjacency_list[u] = []
        self.adjacency_list[u].append(v)
    def print_graph(self):
        for node, neighbors in self.adjacency_list.items():
            print(f"{node}: {', '.join(map(str, neighbors))}")
my_graph = Graph()
my_graph.add_edge(1, 2)
my_graph.add_edge(1, 3)
my_graph.print_graph()
# Output:
# 1: 2, 3
# Heap: A heap is a specialized tree-based data structure that satisfies the heap property. In Python, you can use the heapq module to implement heaps.
# Example:
from email.mime import base
import heapq
my_heap = []
heapq.heappush(my_heap, 2)
heapq.heappush(my_heap, 1)
print(heapq.heappop(my_heap))  # Output: 1

print(my_heap)  # Output: [2]
# Trie: A trie (prefix tree) is a tree-like data structure used to store a dynamic set of strings. You can implement a trie using classes in Python.
# Example:
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end_of_word = False
class Trie:
    def __init__(self):
        self.root = TrieNode()
    def insert(self, word):
        current = self.root
        for char in word:
            if char not in current.children:
                current.children[char] = TrieNode()
            current = current.children[char]
        current.is_end_of_word = True
    def search(self, word):
        current = self.root
        for char in word:
            if char not in current.children:
                return False
            current = current.children[char]
        return current.is_end_of_word
my_trie = Trie()
my_trie.insert("hello")
print(my_trie.search("hello"))  # Output: True
print(my_trie.search("hell"))   # Output: False
# Hash Table: A hash table is a data structure that maps keys to values using a hash function. In Python, you can use dictionaries to implement hash tables.
# Example:
my_hash_table = {}
my_hash_table["key1"] = "value1"
my_hash_table["key2"] = "value2"
print(my_hash_table["key1"])  # Output: value1
print(my_hash_table)  # Output: {'key1': 'value1', 'key2': 'value2'}
# These examples demonstrate how to implement various linear and non-linear data structures in Python. Each data structure has its own use cases and advantages depending on the specific requirements of your application.


########## 3. What is __init__() in Python?
# The __init__() method is known as a constructor in object-oriented programming (OOP) terminology. It is used to initialize an object's state when it is created. This method is automatically called when a new instance of a class is instantiated.

# We have created a book_shop class and added the constructor and book() function. The constructor will store the book title name and the book() function will print the book name.

# To test our code we have initialized the b object with “Sandman” and executed the book() function. 
class book_shop:

    # constructor
    def __init__(self, title):
        self.title = title

    # Sample method
    def book(self):
        print('The tile of the book is', self.title)


b = book_shop('Sandman')
b.book()
# The tile of the book is Sandman


# 10. What is the difference between shallow copy and deep copy in Python, and when would you use each?
# In Python, shallow and deep copies are used to duplicate objects, but they handle nested structures differently.

# Shallow Copy: A shallow copy creates a new object but inserts references to the objects found in the original. So, if the original object contains other mutable objects (like lists within lists), the shallow copy will reference the same inner objects. This can lead to unexpected changes if you modify one of those inner objects in either the original or copied structure. You can create a shallow copy using the copy() method or the copy module’s copy() function.

# Deep Copy: A deep copy creates a new object and recursively copies all objects found within the original. This means that even nested structures get duplicated, so changes in one copy don’t affect the other. To create a deep copy, you can use the copy module’s deepcopy() function.

# Example Usage: A shallow copy is suitable when the object contains only immutable items or when you want changes in nested structures to reflect in both copies. A deep copy is ideal when working with complex, nested objects where you want a completely independent duplicate. Read our Python Copy List: What You Should Know tutorial to learn more. This tutorial includes a whole section on the difference between shallow copy and deep copy. 

# 13. What is the Python “with” statement designed for?
# The with statement is used for exception handling to make code cleaner and simpler. It is generally used for the management of common resources like creating, editing, and saving a file. 

# Example:

# Instead of writing multiple lines of open, try, finally, and close, you can create and write a text file using the with statement. It is simple.

# # using with statement
# with open('myfile.txt', 'w') as file:
#     file.write('DataCamp Black Friday Sale!!!')

# 4. Why use else in try/except construct in Python?
# try: and except: are commonly known for exceptional handling in Python, so where does else: come in handy? else: will be triggered when no exception is raised. 

# Example:

# Let’s learn more about else: with a couple of examples.

# On the first try, we entered 2 as the numerator and d as the denominator. Which is incorrect, and except: was triggered with “Invalid input!”. 

# On the second try, we entered 2 as the numerator and 1 as the denominator and got the result 2. No exception was raised, so it triggered the else: printing the message Division is successful. 

# Metaclasses are classes of classes. They define how classes behave and are created. While regular classes create objects, metaclasses create classes. By using metaclasses, you can modify class definitions, enforce rules, or add functionality during class creation.
# class Meta(type):
#     def __new__(cls, name, bases, dct):
#         print(f"Creating class {name}")
#         return super().__new__(cls, name, bases, dct)

# class MyClass(metaclass=Meta):
#     pass
# # Output: Creating class MyClass


# What is Big O notation?
# Big O notation describes how an algorithm's runtime or space usage grows as input size increases. It helps you compare algorithm efficiency and choose the best approach for your problem.

# Common complexities include O(1) for constant time, O(n) for linear time, and O(nˆ2) for quadratic time. A binary search runs in O(log n) time, which makes it much faster than linear search for large datasets. For example, searching through a million items takes only about 20 steps with binary search versus up to a million steps with linear search.

# You'll also encounter O(n log n) for efficient sorting algorithms like merge sort and O(2^n) for exponential algorithms that quickly become impractical for large inputs.


# Explain the difference between arrays and linked lists
# Arrays store elements in contiguous memory locations with a fixed size, while linked lists use nodes connected by pointers with a dynamic size. Arrays offer O(1) random access but costly insertions. Linked lists provide O(1) insertions but require O(n) time to access specific elements.
# Array access
# arr = [1, 2, 3, 4, 5]
# element = arr[2]  # O(1) access

# # Linked list implementation and usage
# class ListNode:
#    def __init__(self, val=0):
#        self.val = val
#        self.next = None

# # Linked list: 1 -> 2 -> 3
# head = ListNode(1)
# head.next = ListNode(2)
# head.next.next = ListNode(3)

# # Traversing the linked list
# current = head
# while current:
#    print(current.val)  # Prints 1, 2, 3
#    current = current.next

# What is recursion?
# Recursion occurs when a function calls itself to solve smaller versions of the same problem. Every recursive function needs a base case to stop the recursion and a recursive case that moves toward the base case.

# What is the difference between depth-first search and breadth-first search?
# Depth-first search (DFS) explores as far down one branch as possible before backtracking, while breadth-first search (BFS) explores all neighbors at the current level before moving deeper. DFS uses a stack (or recursion), and BFS uses a queue to manage the order of exploration.

# DFS works well for problems like detecting cycles, finding connected components, or exploring all possible paths. It uses less memory when the tree is wide, but it can get stuck in deep branches. BFS guarantees finding the shortest path in unweighted graphs and works better when the solution is likely to be near the starting point.

# Both algorithms have O(V + E) time complexity for graphs, where V is vertices and E is edges. Choose DFS when you need to explore all possibilities or when memory is limited. Choose BFS when finding the shortest path or when solutions are likely to be shallow.

# What is the difference between a process and a thread?
# A process is an independent program in execution with its own memory space, while a thread is a lightweight unit of execution within a process that shares memory with other threads. Processes provide isolation and security but require more resources to create and manage. Threads offer faster creation and communication, but can cause issues when sharing data.

# Process communication happens through inter-process communication (IPC) mechanisms like pipes, shared memory, or message queues. Thread communication is simpler since they share the same address space, but this requires careful synchronization to prevent race conditions and data corruption.

# The choice between processes and threads depends on your specific needs. Use processes when you need isolation, fault tolerance, or want to utilize multiple CPU cores for CPU-intensive tasks. Use threads for I/O-bound tasks, when you need fast communication, or when working within memory constraints.

# Database indexes are data structures that improve query performance by creating shortcuts to data rows. Clustered indexes determine the physical storage order of data, with each table having at most one clustered index. Non-clustered indexes create separate structures that point to data rows, allowing multiple indexes per table.

# B-tree indexes work well for range queries and equality searches, making them the default choice for most databases. Hash indexes provide O(1) lookup for equality comparisons but can't handle range queries. Bitmap indexes work efficiently for low-cardinality data like gender or status fields, especially in data warehouses.

# Composite indexes cover multiple columns and can significantly speed up queries that filter on multiple fields. However, indexes require additional storage space and slow down insert, update, and delete operations because the database must maintain index consistency. Choose indexes carefully based on your query patterns and performance requirements.



# How would you design a distributed caching system like Redis?
# A distributed caching system requires much consideration of data partitioning, consistency, and fault tolerance. The core challenge is distributing data across multiple nodes while maintaining fast access times and handling node failures gracefully. Consistent hashing provides an elegant solution by minimizing data movement when nodes are added or removed from the cluster.

# The system needs to handle cache eviction policies, data replication, and network partitions. Implement a ring-based architecture where each key maps to a position on the ring, and the responsible node is the first one encountered moving clockwise. Use virtual nodes to ensure better load distribution and reduce hotspots. For fault tolerance, replicate data to N successor nodes and implement read/write quorums to maintain availability during failures.

# Memory management becomes critical at scale, requiring sophisticated eviction algorithms beyond simple LRU. Consider approximated LRU using sampling, or implement adaptive replacement caches that balance recency and frequency. Add features like data compression, TTL management, and monitoring for cache hit rates and memory usage. The system should support both synchronous and asynchronous replication depending on consistency requirements.

# Explain the CAP theorem and its implications for distributed systems
# The CAP theorem states that distributed systems can guarantee at most two of three properties: Consistency (all nodes see the same data simultaneously), Availability (system remains operational), and Partition tolerance (system continues despite network failures). This fundamental limitation forces architects to make explicit trade-offs when designing distributed systems.

# In practice, partition tolerance is non-negotiable for distributed systems since network failures are inevitable. This leaves you choosing between consistency and availability during partitions. CP systems like traditional databases prioritize consistency and may become unavailable during network splits. AP systems, like many NoSQL databases, remain available but may serve stale data until the partition heals.

# Modern systems often implement eventual consistency, where the system becomes consistent over time rather than immediately. CRDT (Conflict-free Replicated Data Types) and vector clocks help manage consistency in AP systems. Some systems use different consistency models for different operations — strong consistency for critical data like financial transactions, and eventual consistency for less critical data like user preferences or social media posts.

# > Understanding the components and applications of distributed computing can enhance your system design skills. Learn more in our article on Distributed Computing.

# How do you implement a rate limiter for an API?
# Rate limiting protects APIs from abuse and ensures fair resource usage across clients. The most common algorithms are token bucket, leaky bucket, fixed window, and sliding window. Token bucket allows bursts up to the bucket size while maintaining an average rate, making it ideal for APIs that need to handle occasional spikes while preventing sustained abuse.

# Implement rate limiting at multiple levels: per-user, per-IP, per-API key, and global limits. Use Redis or another fast data store to track rate limit counters with appropriate expiration times. For high-scale systems, consider distributed rate limiting where multiple API gateway instances coordinate through shared storage. Implement different limits for different user tiers and API endpoints based on their computational cost.

# Handle rate limit violations gracefully by returning appropriate HTTP status codes (429 Too Many Requests) with retry-after headers. Provide clear error messages and consider implementing queue-based processing for non-urgent requests. Advanced implementations include dynamic rate limiting that adjusts based on system load, and rate limiting bypass for critical operations during emergencies.


# How would you design a database sharding strategy?
# Database sharding distributes data across multiple databases to handle loads that exceed a single database's capacity. The sharding key determines how data gets distributed and significantly impacts query performance and scalability. Choose keys that distribute data evenly while keeping related data together to minimize cross-shard queries.

# Horizontal sharding splits rows across shards based on a sharding function, while vertical sharding separates tables or columns. Range-based sharding uses value ranges (user IDs 1-1000 on shard 1), which works well for time-series data but can create hotspots. Hash-based sharding distributes data more evenly but makes range queries difficult. Directory-based sharding uses a lookup service to map keys to shards, providing flexibility at the cost of an additional lookup.

# Plan for shard rebalancing as data grows unevenly across shards. Implement a shard management layer that handles routing, connection pooling, and cross-shard operations. Consider using database proxies or middleware that abstract sharding complexity from applications. For complex queries spanning multiple shards, implement scatter-gather patterns or maintain denormalized views. Monitor shard utilization and implement automated splitting or merging based on predefined thresholds.

# Explain microservices architecture and when to use it
# Microservices architecture decomposes applications into small, independent services that communicate through well-defined APIs. Each service owns its data, can be developed and deployed independently, and typically focuses on a single business capability. This approach enables teams to work autonomously, use different technologies, and scale services independently based on demand.

# The main benefits include improved fault isolation, technology diversity, and independent deployment cycles. When one service fails, others continue operating. Teams can choose the best tools for their specific problems and deploy updates without coordinating with other teams. However, microservices introduce complexity in service discovery, distributed tracing, data consistency, and network communication that doesn't exist in monolithic applications.

# Consider microservices when you have a large team, complex domain requirements, or need to scale different parts of your system independently. Avoid them for simple applications, small teams, or when you're still exploring the problem domain. Start with a monolith and extract services as boundaries become clear. Successful microservices require strong DevOps practices, monitoring infrastructure, and organizational maturity to handle the distributed system complexity.

# How do you handle eventual consistency in distributed systems?
# Eventual consistency guarantees that if no new updates occur, all replicas will eventually converge to the same value. This model trades immediate consistency for availability and partition tolerance, making it suitable for systems that can tolerate temporary inconsistencies. Implement eventual consistency through conflict resolution strategies, versioning, and careful application design.

# Vector clocks or version vectors help track causality between events in distributed systems. Each replica maintains a logical clock that increments with local updates and gets updated when receiving remote updates. When conflicts occur, the system can detect concurrent updates and apply resolution strategies like last-writer-wins, user-defined merge functions, or presenting conflicts to users for manual resolution.

# Design your application to handle inconsistent states gracefully. Use compensating transactions to correct inconsistencies, implement idempotent operations to handle duplicate messages, and design UIs that can display pending or conflicting states. Consider using CRDT (Conflict-free Replicated Data Types) for data structures that can merge automatically without conflicts, such as counters, sets, and collaborative documents.

# Explain the principles of distributed database design
# Distributed databases face unique challenges in maintaining consistency, availability, and partition tolerance while providing acceptable performance. Design principles include data partitioning strategies, replication models, and transaction management across multiple nodes. Horizontal partitioning (sharding) distributes rows across nodes, while vertical partitioning separates columns or tables.

# Replication strategies balance consistency and availability requirements. Synchronous replication ensures consistency but may impact availability during network issues. Asynchronous replication maintains availability but risks data loss during failures. Multi-master replication allows writes to multiple nodes but requires sophisticated conflict resolution. Consider using different replication strategies for different data types based on their consistency requirements.

# Implement distributed transaction protocols like two-phase commit for operations spanning multiple nodes, but understand their blocking behavior during failures. Modern systems often prefer eventual consistency with compensation patterns over distributed transactions. Design your schema and query patterns to minimize cross-partition operations, and implement monitoring for query performance, replication lag, and partition utilization.

# How do you design for fault tolerance and disaster recovery?
# Fault tolerance requires redundancy at every system level — hardware, software, network, and data. Implement the principle of "assume everything will fail" by designing systems that gracefully handle component failures without impacting user experience. Use redundant servers, load balancers, network paths, and data centers to eliminate single points of failure.

# Design circuit breakers to prevent cascading failures when downstream services become unavailable. Implement bulkhead patterns to isolate different system components, ensuring that failure in one area doesn't bring down the entire system. Use timeouts, retries with exponential backoff, and graceful degradation to handle temporary failures. Monitor system health continuously and implement automated failover mechanisms.

# Disaster recovery planning involves regular backups, geographically distributed infrastructure, and tested recovery procedures. Implement Recovery Time Objective (RTO) and Recovery Point Objective (RPO) requirements based on business needs. Use database replication across regions, automated backup verification, and regular disaster recovery drills. Consider chaos engineering practices to proactively identify failure modes and improve system resilience before they impact production.
